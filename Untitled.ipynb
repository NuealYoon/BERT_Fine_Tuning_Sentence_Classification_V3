{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "## 1.1. Using Colab GPU for Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cd5bb073bb20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GT 745M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoon/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py:87: UserWarning: \n",
      "    Found GPU0 GeForce GT 745M which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability that we support is 3.5.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Installing the Hugging Face Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement transformer (from versions: none)\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for transformer\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading CoLA Dataset\n",
    "## 2.1. Download & Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "print('Downloading dataset...')\n",
    "\n",
    "# The URL for dataset zip file.\n",
    "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
    "\n",
    "# Download the file (if we haven't already)\n",
    "if not os.path.exists('./cola_public_1.1.zip'):\n",
    "    wget.download(url, './cola_public_1.1.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  cola_public_1.1.zip\r\n",
      "   creating: cola_public/\r\n",
      "  inflating: cola_public/README      \r\n",
      "   creating: cola_public/tokenized/\r\n",
      "  inflating: cola_public/tokenized/in_domain_dev.tsv  \r\n",
      "  inflating: cola_public/tokenized/in_domain_train.tsv  \r\n",
      "  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \r\n",
      "   creating: cola_public/raw/\r\n",
      "  inflating: cola_public/raw/in_domain_dev.tsv  \r\n",
      "  inflating: cola_public/raw/in_domain_train.tsv  \r\n",
      "  inflating: cola_public/raw/out_of_domain_dev.tsv  \r\n"
     ]
    }
   ],
   "source": [
    "# Unzip the dataset (if we haven't already)\n",
    "if not os.path.exists('./cola_public/'):\n",
    "    !unzip cola_public_1.1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Parse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 8,551\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3524</td>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The birds are singing because the weather is l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2896</td>\n",
       "      <td>l-93</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I broke the twig off the branch.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015</td>\n",
       "      <td>bc01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Some tourists visited all the museums.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6318</td>\n",
       "      <td>c_13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chris wants himself to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3865</td>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Frank threw himself into the sofa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4572</td>\n",
       "      <td>ks08</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>John seems fond of ice cream, and Bill seems, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7964</td>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>He might no could have done it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1951</td>\n",
       "      <td>r-67</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John and Mary met in Vienna.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7133</td>\n",
       "      <td>sks13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This girl in the red coat will put a picture o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7724</td>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I might have eaten some seaweed.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_source  label label_notes  \\\n",
       "3524            ks08      1         NaN   \n",
       "2896            l-93      1         NaN   \n",
       "1015            bc01      1         NaN   \n",
       "6318            c_13      1         NaN   \n",
       "3865            ks08      1         NaN   \n",
       "4572            ks08      0           *   \n",
       "7964            ad03      1         NaN   \n",
       "1951            r-67      1         NaN   \n",
       "7133           sks13      1         NaN   \n",
       "7724            ad03      1         NaN   \n",
       "\n",
       "                                               sentence  \n",
       "3524  The birds are singing because the weather is l...  \n",
       "2896                   I broke the twig off the branch.  \n",
       "1015             Some tourists visited all the museums.  \n",
       "6318                        Chris wants himself to win.  \n",
       "3865                 Frank threw himself into the sofa.  \n",
       "4572  John seems fond of ice cream, and Bill seems, ...  \n",
       "7964                     He might no could have done it  \n",
       "1951                       John and Mary met in Vienna.  \n",
       "7133  This girl in the red coat will put a picture o...  \n",
       "7724                   I might have eaten some seaweed.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two properties we actually care about are the the sentence and its label, which is referred to as the \"acceptibility judgment\" (0=unacceptable, 1=acceptable).\n",
    "\n",
    "우리가 실제로 관심을 갖는 두 가지 속성은 문장과 그 레이블이며, \"허용 가능성 판단\"(0 = unacceptable, 1 = acceptable)이라고합니다.\n",
    "\n",
    "\n",
    "Here are five sentences which are labeled as not grammatically acceptible. Note how much more difficult this task is than something like sentiment analysis!\n",
    "\n",
    "문법적으로 허용되지 않는 것으로 표시된 5 개의 문장이 있습니다. 이 작업이 감정 분석과 같은 것보다 얼마나 어려운지 주목하십시오!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1407</td>\n",
       "      <td>Light was made of her indiscretions.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>The more obnoxious Fred, the less attention yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>He gets angry, the longer John has to wait.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>583</td>\n",
       "      <td>The cup filled of water.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1279</td>\n",
       "      <td>The tall nurse who Tony has a Fiat and yearns ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label\n",
       "1407               Light was made of her indiscretions.      0\n",
       "171   The more obnoxious Fred, the less attention yo...      0\n",
       "248         He gets angry, the longer John has to wait.      0\n",
       "583                            The cup filled of water.      0\n",
       "1279  The tall nurse who Tony has a Fiat and yearns ...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Our friends won't buy this analysis, let alone the next one we propose.\"\n",
      " \"One more pseudo generalization and I'm giving up.\"\n",
      " \"One more pseudo generalization or I'm giving up.\" ...\n",
      " 'It is easy to slay the Gorgon.'\n",
      " 'I had the strangest feeling that I knew you.'\n",
      " 'What all did you get for Christmas?']\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values\n",
    "\n",
    "print(sentences)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tokenization & Input Formatting\n",
    "\n",
    "## 3.1. BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
      "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
      "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Required Formatting\n",
    "\n",
    "The above code left out a few required formatting steps that we'll look at here.\n",
    "위의 코드는 여기서 살펴볼 몇 가지 필수 형식 지정 단계를 생략했습니다.\n",
    "\n",
    "Side Note: The input format to BERT seems \"over-specified\" to me... We are required to give it a number of pieces of information which seem redundant, or like they could easily be inferred from the data without us explicity providing it. But it is what it is, and I suspect it will make more sense once I have a deeper understanding of the BERT internals.\n",
    "참고 : BERT의 입력 형식은 나에게 \"과도하게 지정되어있는\"것 같습니다. 중복 된 것처럼 보이거나 데이터를 명시 적으로 제공하지 않고 쉽게 데이터에서 유추 할 수있는 많은 정보를 제공해야합니다. . 그러나 그것은 그것이 무엇이며, BERT 내부에 대해 깊이 이해하면 더 이해가 될 것입니다.\n",
    "\n",
    "We are required to:\n",
    "\n",
    "1. Add special tokens to the start and end of each sentence.\n",
    "2. Pad & truncate all sentences to a single constant length.\n",
    "3. Explicitly differentiate real tokens from padding tokens with the \"attention mask\".\n",
    "\n",
    "1. 각 문장의 시작과 끝에 특수 토큰을 추가하십시오.\n",
    "2. 모든 문장을 하나의 일정한 길이로 채 웁니다.\n",
    "3. \"attention mask\"를 사용하여 실제 토큰과 패딩 토큰을 명시 적으로 구별하십시오.\n",
    "\n",
    "\n",
    "### Special Tokens\n",
    "\n",
    "[SEP]\n",
    "\n",
    "At the end of every sentence, we need to append the special [SEP] token.\n",
    "모든 문장 끝에 특별한 [SEP] 토큰을 추가해야합니다.\n",
    "\n",
    "This token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine something (e.g., can the answer to the question in sentence A be found in sentence B?).\n",
    "이 토큰은 BERT에 두 개의 개별 문장이 제공되고 무언가를 결정하도록 요청되는 2 문장 작업의 인공물입니다 (예 : 문장 A의 질문에 대한 답변을 문장 B에서 찾을 수 있습니까?).\n",
    "\n",
    "I am not certain yet why the token is still required when we have only single-sentence input, but it is!\n",
    "한 문장 만 입력해도 여전히 토큰이 필요한 이유는 확실하지 않습니다.\n",
    "\n",
    "[CLS]\n",
    "\n",
    "For classification tasks, we must prepend the special [CLS] token to the beginning of every sentence.\n",
    "분류 작업의 경우 모든 문장의 시작 부분에 특수 [CLS] 토큰을 추가해야합니다.\n",
    "\n",
    "\n",
    "This token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output (but with the feature values changed, of course!).\n",
    "이 토큰에는 특별한 의미가 있습니다. BERT는 12 개의 트랜스포머 레이어로 구성됩니다. 각 변환기는 토큰 임베딩 목록을 가져와 출력에 동일한 수의 임베딩을 생성합니다 (물론 기능 값이 변경됨).\n",
    "\n",
    "![Illustration of CLS token purpose](http://www.mccormickml.com/assets/BERT/CLS_token_500x606.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "On the output of the final (12th) transformer, *only the first embedding (corresponding to the [CLS] token) is used by the classifier*.\n",
    "\n",
    ">  \"The first token of every sequence is always a special classification token (`[CLS]`). The final hidden state\n",
    "corresponding to this token is used as the aggregate sequence representation for classification\n",
    "tasks.\" (from the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "\n",
    "You might think to try some pooling strategy over the final embeddings, but this isn't necessary. Because BERT is trained to only use this [CLS] token for classification, we know that the model has been motivated to encode everything it needs for the classification step into that single 768-value embedding vector. It's already done the pooling for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
